{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the working directory to your project's main folder\n",
    "os.chdir('/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/')\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from utils.ml_logging import get_logger\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = os.getenv('KEY')\n",
    "REGION = os.getenv('REGION')\n",
    "FILE_NAME = '/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/8000khz-mulaw-pullstream/7.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_file_async(file_name: str, key: str, region: str) -> str:\n",
    "    \"\"\"\n",
    "    Transcribes speech from an audio file using Azure Cognitive Services Speech SDK.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the audio file to transcribe.\n",
    "        key (str): The subscription key for the Speech service.\n",
    "        region (str): The region for the Speech service.\n",
    "\n",
    "    Returns:\n",
    "        str: The transcribed text from the audio file.\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key, region=region)\n",
    "    audio_config = speechsdk.AudioConfig(filename=file_name)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    logger.info(f\"Transcribing speech from file: {file_name}\")\n",
    "    result = speech_recognizer.recognize_once_async().get()\n",
    "    if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        logger.info(f\"Transcription result: {result.text}\")\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        logger.warning(f\"No speech could be recognized: {result.no_match_details}\")\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        logger.error(f\"Speech Recognition canceled: {cancellation_details.reason}\")\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            logger.error(f\"Error details: {cancellation_details.error_details}\")\n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def from_file_continous(file_name: str, key: str, region: str) -> str:\n",
    "    \"\"\"performs continuous speech recognition with input from an audio file\"\"\"\n",
    "    # Set up logging\n",
    "  \n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key, region=region)\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=file_name)\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    done = False\n",
    "    final_text = \"\"\n",
    "\n",
    "    def update_final_text(evt):\n",
    "        nonlocal final_text\n",
    "        final_text += ' ' + evt.result.text\n",
    "\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        logger.info('CLOSING on {}'.format(evt))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    # Connect callbacks to the events fired by the speech recognizer\n",
    "    speech_recognizer.recognizing.connect(lambda evt: logger.info('RECOGNIZING: {}'.format(evt)))\n",
    "    speech_recognizer.recognized.connect(update_final_text)\n",
    "    speech_recognizer.session_started.connect(lambda evt: logger.info('SESSION STARTED: {}'.format(evt)))\n",
    "    speech_recognizer.session_stopped.connect(lambda evt: logger.info('SESSION STOPPED {}'.format(evt)))\n",
    "    speech_recognizer.canceled.connect(lambda evt: logger.info('CANCELED {}'.format(evt)))\n",
    "    # Stop continuous recognition on either session stopped or canceled events\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start continuous speech recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.1)\n",
    "\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "    return final_text.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 17:08:54,221 - micro - MainProcess - INFO     SESSION STARTED: SessionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00) (2284356991.py:<lambda>:27)\n",
      "2023-11-14 17:08:54,786 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=112efca37baa47cf817054a6f480b787, text=\"what is the date\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:08:55,089 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=0336bf60e307403ea36a5f2f5e4d1e0e, text=\"may 15th\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:08:55,164 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=e77aeee7a6324e9e85a3809b97a2c088, text=\"may 15th 1980\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:08:56,321 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=90b907b42ae54c218701a079c51d289a, text=\"thursday\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:08:56,817 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=7254b3d8058e456c8ccdaf274569966f, text=\"thursday may\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:08:57,114 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=8fd77209ea41462abda57a8c76054b96, text=\"thursday may 15th\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:08:57,522 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=e4e4266bcb5243849de6e06e705ac12f, text=\"thursday may 15th 1900\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:08:57,722 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=ca81e6619d9f4a3e84766696f0c990cd, text=\"thursday may 15th 19180\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:08:58,688 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=dd3cb4f0bb904461828d074c2274823e, text=\"what is\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:08:58,783 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=b38f307a53af4671bc7f0db4e0e4b760, text=\"what is the date\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:09:00,215 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=c2adcd76dd1544dfb0028bb4789351ce, text=\"july 6\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:09:02,092 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=6ee476ae224b4de88d3d4c2cff27c90e, text=\"saturday\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:09:02,290 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=f40f69076c924d6f8111c30054fd3962, text=\"saturday july\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:09:02,694 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=cd880503eeaf401b9bca035d301b8984, text=\"saturday july 6th\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:09:02,994 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=9b8fc3b97778459dba10692ba05261c8, text=\"saturday july 6th 2:00\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:09:03,098 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=c499b464a47a4833b87546878c0cbc04, text=\"saturday july 6th 2000\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:09:03,298 - micro - MainProcess - INFO     RECOGNIZING: SpeechRecognitionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=b2062f3f765c463f957d4855a20de7d9, text=\"saturday july 6th 2020\", reason=ResultReason.RecognizingSpeech)) (2284356991.py:<lambda>:25)\n",
      "2023-11-14 17:09:03,311 - micro - MainProcess - INFO     CANCELED SpeechRecognitionCanceledEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=b311af37317c44c2a689e7c446cea5c1, text=\"\", reason=ResultReason.Canceled)) (2284356991.py:<lambda>:29)\n",
      "2023-11-14 17:09:03,312 - micro - MainProcess - INFO     CLOSING on SpeechRecognitionCanceledEventArgs(session_id=95661a320c8b472a9f9079458e24cf00, result=SpeechRecognitionResult(result_id=b311af37317c44c2a689e7c446cea5c1, text=\"\", reason=ResultReason.Canceled)) (2284356991.py:stop_cb:20)\n",
      "2023-11-14 17:09:03,314 - micro - MainProcess - INFO     SESSION STOPPED SessionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00) (2284356991.py:<lambda>:28)\n",
      "2023-11-14 17:09:03,316 - micro - MainProcess - INFO     CLOSING on SessionEventArgs(session_id=95661a320c8b472a9f9079458e24cf00) (2284356991.py:stop_cb:20)\n"
     ]
    }
   ],
   "source": [
    "text = from_file_continous(file_name=FILE_NAME, key=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the date? May 15th, 1980. Thursday, May 15th, 19180. What is the date? July 6th. Saturday, July 6th, 2024.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 16:46:38,782 - micro - MainProcess - INFO     Transcribing speech from file: /mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/8000khz-mulaw-pullstream/7.wav (248621148.py:from_file:17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 16:46:39,395 - micro - MainProcess - INFO     Transcription result: What is the date? (248621148.py:from_file:20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the date?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_file(file_name=FILE_NAME, key=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config.enable_audio_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = speechsdk.AudioConfig(filename=FILE_NAME)\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mspeech_recognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mazure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcognitiveservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpeechRecognitionResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Performs recognition in a blocking (synchronous) mode. Returns after a single utterance is\n",
      "recognized. The end of a single utterance is determined by listening for silence at the end\n",
      "or until a maximum of 15 seconds of audio is processed. The task returns the recognition\n",
      "text as result. For long-running multi-utterance recognition, use\n",
      ":py:meth:`.start_continuous_recognition_async` instead.\n",
      "\n",
      ":returns: The result value of the synchronous recognition.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/speech.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "result = speech_recognizer.recognize_once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the date?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Id\":\"0a812f792b094255b31eb6e1abf81c88\",\"RecognitionStatus\":\"Success\",\"DisplayText\":\"May 15th, 1980.\",\"Offset\":38600000,\"Duration\":16000000,\"Channel\":0}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the date?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time \n",
    "import os\n",
    "from utils.ml_logging import get_logger\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "KEY = os.getenv('KEY')\n",
    "REGION = os.getenv('REGION')\n",
    "FILE_NAME = '/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/8000khz-mulaw-pullstream/7.wav'\n",
    "language_understanding_app_id = os.getenv('INTENT_KEY')\n",
    "\n",
    "\n",
    "\n",
    "intent_config: speechsdk.SpeechConfig = speechsdk.SpeechConfig(subscription=KEY, region=REGION)\n",
    "audio_config: speechsdk.audio.AudioConfig = speechsdk.audio.AudioConfig(filename=FILE_NAME)\n",
    "intent_recognizer: speechsdk.intent.IntentRecognizer = speechsdk.intent.IntentRecognizer(speech_config=intent_config, audio_config=audio_config)\n",
    "\n",
    "# set up the intents that are to be recognized. These can be a mix of simple phrases and\n",
    "# intents specified through a LanguageUnderstanding Model.\n",
    "model = speechsdk.intent.LanguageUnderstandingModel(app_id=language_understanding_app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_intent_continuous(file_name: str, key: str, region: str) -> None:\n",
    "    \"\"\"\n",
    "    Performs continuous intent recognition from input from an audio file.\n",
    "    Uses the Azure Cognitive Services Speech SDK to set up an intent recognizer,\n",
    "    add intents to be recognized, and start continuous recognition.\n",
    "    Prints the output of the recognition to the console.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the audio file to transcribe.\n",
    "        key (str): The subscription key for the Speech service.\n",
    "        region (str): The region for the Speech service.\n",
    "    \"\"\"\n",
    "    # Set up the intent recognizer\n",
    "    intent_config: speechsdk.SpeechConfig = speechsdk.SpeechConfig(subscription=key, region=region)\n",
    "    audio_config: speechsdk.audio.AudioConfig = speechsdk.audio.AudioConfig(filename=file_name)\n",
    "    intent_recognizer: speechsdk.intent.IntentRecognizer = speechsdk.intent.IntentRecognizer(speech_config=intent_config, audio_config=audio_config)\n",
    "\n",
    "    # set up the intents that are to be recognized. These can be a mix of simple phrases and\n",
    "    # intents specified through a LanguageUnderstanding Model.\n",
    "    model = speechsdk.intent.LanguageUnderstandingModel(app_id=language_understanding_app_id)\n",
    "    intents = [\n",
    "        (model, \"HomeAutomation.TurnOn\"),\n",
    "        (model, \"HomeAutomation.TurnOff\"),\n",
    "        (\"This is a test.\", \"test\"),\n",
    "        (\"Switch the channel to 34.\", \"34\"),\n",
    "        (\"what's the weather like\", \"weather\"),\n",
    "    ]\n",
    "    intent_recognizer.add_intents(intents)\n",
    "\n",
    "    # Connect callback functions to the signals the intent recognizer fires.\n",
    "    done = False\n",
    "\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    intent_recognizer.session_started.connect(lambda evt: print(\"SESSION_START: {}\".format(evt)))\n",
    "    intent_recognizer.speech_end_detected.connect(lambda evt: print(\"SPEECH_END_DETECTED: {}\".format(evt)))\n",
    "    # event for intermediate results\n",
    "    intent_recognizer.recognizing.connect(lambda evt: print(\"RECOGNIZING: {}\".format(evt)))\n",
    "    # event for final result\n",
    "    intent_recognizer.recognized.connect(lambda evt: print(\n",
    "        \"RECOGNIZED: {}\\n\\tText: {} (Reason: {})\\n\\tIntent Id: {}\\n\\tIntent JSON: {}\".format(\n",
    "            evt, evt.result.text, evt.result.reason, evt.result.intent_id, evt.result.intent_json)))\n",
    "\n",
    "    # cancellation event\n",
    "    intent_recognizer.canceled.connect(lambda evt: print(f\"CANCELED: {evt.cancellation_details} ({evt.reason})\"))\n",
    "\n",
    "    # stop continuous recognition on session stopped, end of speech or canceled events\n",
    "    intent_recognizer.session_stopped.connect(stop_cb)\n",
    "    intent_recognizer.speech_end_detected.connect(stop_cb)\n",
    "    intent_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # And finally run the intent recognizer. The output of the callbacks should be printed to the console.\n",
    "    intent_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.5)\n",
    "\n",
    "    intent_recognizer.stop_continuous_recognition()\n",
    "    # </IntentContinuousRecognitionWithFile>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SESSION_START: SessionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=38a249704fe3412385174092ed0eb4f1, text=\"what is the\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "SPEECH_END_DETECTED: RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "CLOSING on RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "RECOGNIZED: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=04c7e8e4b9c747439abfaa2cc6183ecc, text=\"What is the date?\", intent_id=, reason=ResultReason.RecognizedSpeech))\n",
      "\tText: What is the date? (Reason: ResultReason.RecognizedSpeech)\n",
      "\tIntent Id: \n",
      "\tIntent JSON: \n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=69f1ceb0bf0b43bebe3a27ef0903d909, text=\"may\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=01e2938ae8014c07991541e4eccb20d6, text=\"may 15th\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=6aa6aa3cb71e439889fc4187d33487c8, text=\"may 15th 1980\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "SPEECH_END_DETECTED: RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "CLOSING on RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "RECOGNIZED: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=7dad3d545cac44508b20a6feb210d61e, text=\"May 15th, 1980.\", intent_id=, reason=ResultReason.RecognizedSpeech))\n",
      "\tText: May 15th, 1980. (Reason: ResultReason.RecognizedSpeech)\n",
      "\tIntent Id: \n",
      "\tIntent JSON: \n",
      "CLOSING on SessionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n"
     ]
    }
   ],
   "source": [
    "recognize_intent_continuous(file_name=FILE_NAME, key=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = [\n",
    "        (model, \"HomeAutomation.TurnOn\"),\n",
    "        (model, \"HomeAutomation.TurnOff\"),\n",
    "        (\"This is a test.\", \"test\"),\n",
    "        (\"Switch the channel to 34.\", \"34\"),\n",
    "        (\"what's the weather like\", \"weather\"),\n",
    "    ]\n",
    "intent_recognizer.add_intents(intents)\n",
    "\n",
    "# Connect callback functions to the signals the intent recognizer fires.\n",
    "done = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "    \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "    print('CLOSING on {}'.format(evt))\n",
    "    nonlocal done\n",
    "    done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "intent_recognizer.session_started.connect(lambda evt: print(\"SESSION_START: {}\".format(evt)))\n",
    "intent_recognizer.speech_end_detected.connect(lambda evt: print(\"SPEECH_END_DETECTED: {}\".format(evt)))\n",
    "# event for intermediate results\n",
    "intent_recognizer.recognizing.connect(lambda evt: print(\"RECOGNIZING: {}\".format(evt)))\n",
    "# event for final result\n",
    "intent_recognizer.recognized.connect(lambda evt: print(\n",
    "    \"RECOGNIZED: {}\\n\\tText: {} (Reason: {})\\n\\tIntent Id: {}\\n\\tIntent JSON: {}\".format(\n",
    "        evt, evt.result.text, evt.result.reason, evt.result.intent_id, evt.result.intent_json)))\n",
    "\n",
    "# cancellation event\n",
    "intent_recognizer.canceled.connect(lambda evt: print(f\"CANCELED: {evt.cancellation_details} ({evt.reason})\"))\n",
    "\n",
    "# stop continuous recognition on session stopped, end of speech or canceled events\n",
    "intent_recognizer.session_stopped.connect(stop_cb)\n",
    "intent_recognizer.speech_end_detected.connect(stop_cb)\n",
    "intent_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "# And finally run the intent recognizer. The output of the callbacks should be printed to the console.\n",
    "intent_recognizer.start_continuous_recognition()\n",
    "while not done:\n",
    "    time.sleep(.5)\n",
    "\n",
    "intent_recognizer.stop_continuous_recognition()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('my-template-environment')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7cde2d05b67b86bafb567671a494f38119b1228112c72ed3f4909b1daf51dd2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
