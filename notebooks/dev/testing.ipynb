{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the working directory to your project's main folder\n",
    "os.chdir(r'c:\\Users\\pablosal\\Desktop\\lilly-workshop-gbb-text-to-speach')\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from utils.ml_logging import get_logger\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.speach_sdk.speach_recognizer import recognize_from_microphone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "SPEECH_KEY = os.getenv('SPEECH_KEY')\n",
    "SPEECH_REGION = os.getenv('SPEECH_REGION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SPEECH_REGION)\n",
    "speech_config.speech_recognition_language=\"en-US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "logger.info(\"Speak into your microphone.\")\n",
    "speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "    logger.info(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "    logger.warning(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = speech_recognition_result.cancellation_details\n",
    "    logger.error(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        logger.error(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "        logger.error(\"Did you set the speech resource key and region values?\")\n",
    "\n",
    "# Return the recognized text and the result object\n",
    "return speech_recognition_result.text, speech_recognition_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception with error code: \n[CALL STACK BEGIN]\n\n    > CreateModuleObject\n    - CreateModuleObject\n    - audio_config_get_audio_processing_options\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - audio_config_get_audio_processing_options\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n\n[CALL STACK END]\n\nException with an error code: 0xe (SPXERR_MIC_NOT_AVAILABLE)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\pablosal\\Desktop\\lilly-workshop-gbb-text-to-speach\\notebooks\\dev\\testing.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m SPEECH_KEY \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m'\u001b[39m\u001b[39mSPEECH_KEY\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m SPEECH_REGION \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mgetenv(\u001b[39m'\u001b[39m\u001b[39mSPEECH_REGION\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m prompt, _ \u001b[39m=\u001b[39m recognize_from_microphone(SPEECH_KEY,SPEECH_REGION)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\Desktop\\lilly-workshop-gbb-text-to-speach\\src\\speach_sdk\\speach_recognizer.py:29\u001b[0m, in \u001b[0;36mrecognize_from_microphone\u001b[1;34m(key, region)\u001b[0m\n\u001b[0;32m     26\u001b[0m speech_config\u001b[39m.\u001b[39mspeech_recognition_language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39men-US\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     28\u001b[0m audio_config \u001b[39m=\u001b[39m speechsdk\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mAudioConfig(use_default_microphone\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m---> 29\u001b[0m speech_recognizer \u001b[39m=\u001b[39m speechsdk\u001b[39m.\u001b[39;49mSpeechRecognizer(speech_config\u001b[39m=\u001b[39;49mspeech_config, audio_config\u001b[39m=\u001b[39;49maudio_config)\n\u001b[0;32m     31\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mSpeak into your microphone.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m speech_recognition_result \u001b[39m=\u001b[39m speech_recognizer\u001b[39m.\u001b[39mrecognize_once_async()\u001b[39m.\u001b[39mget()\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\lilly-speach-to-text\\lib\\site-packages\\azure\\cognitiveservices\\speech\\speech.py:1006\u001b[0m, in \u001b[0;36mSpeechRecognizer.__init__\u001b[1;34m(self, speech_config, audio_config, language, source_language_config, auto_detect_source_language_config)\u001b[0m\n\u001b[0;32m   1004\u001b[0m audio_config_handle \u001b[39m=\u001b[39m audio_config\u001b[39m.\u001b[39m_handle \u001b[39mif\u001b[39;00m audio_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1005\u001b[0m \u001b[39mif\u001b[39;00m language \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m source_language_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m auto_detect_source_language_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1006\u001b[0m     _call_hr_fn(\n\u001b[0;32m   1007\u001b[0m         fn\u001b[39m=\u001b[39;49m_sdk_lib\u001b[39m.\u001b[39;49mrecognizer_create_speech_recognizer_from_config,\n\u001b[0;32m   1008\u001b[0m         \u001b[39m*\u001b[39;49m[ctypes\u001b[39m.\u001b[39;49mbyref(handle), speech_config\u001b[39m.\u001b[39;49m_handle, audio_config_handle])\n\u001b[0;32m   1009\u001b[0m \u001b[39melif\u001b[39;00m language \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1010\u001b[0m     source_language_config \u001b[39m=\u001b[39m languageconfig\u001b[39m.\u001b[39mSourceLanguageConfig(language)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\lilly-speach-to-text\\lib\\site-packages\\azure\\cognitiveservices\\speech\\interop.py:62\u001b[0m, in \u001b[0;36m_call_hr_fn\u001b[1;34m(fn, *args)\u001b[0m\n\u001b[0;32m     60\u001b[0m fn\u001b[39m.\u001b[39mrestype \u001b[39m=\u001b[39m _spx_hr\n\u001b[0;32m     61\u001b[0m hr \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m fn()\n\u001b[1;32m---> 62\u001b[0m _raise_if_failed(hr)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\lilly-speach-to-text\\lib\\site-packages\\azure\\cognitiveservices\\speech\\interop.py:55\u001b[0m, in \u001b[0;36m_raise_if_failed\u001b[1;34m(hr)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_if_failed\u001b[39m(hr: _spx_hr):\n\u001b[0;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m hr \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 55\u001b[0m         __try_get_error(_spx_handle(hr))\n\u001b[0;32m     56\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(hr)\n",
      "File \u001b[1;32mc:\\Users\\pablosal\\AppData\\Local\\anaconda3\\envs\\lilly-speach-to-text\\lib\\site-packages\\azure\\cognitiveservices\\speech\\interop.py:50\u001b[0m, in \u001b[0;36m__try_get_error\u001b[1;34m(error_handle)\u001b[0m\n\u001b[0;32m     45\u001b[0m message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mException with error code: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[0;32m     46\u001b[0m     callstack \u001b[39mif\u001b[39;00m callstack \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     47\u001b[0m     what \u001b[39mif\u001b[39;00m what \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m code\n\u001b[0;32m     48\u001b[0m )\n\u001b[0;32m     49\u001b[0m _sdk_lib\u001b[39m.\u001b[39merror_release(error_handle)\n\u001b[1;32m---> 50\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(message)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Exception with error code: \n[CALL STACK BEGIN]\n\n    > CreateModuleObject\n    - CreateModuleObject\n    - audio_config_get_audio_processing_options\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - audio_config_get_audio_processing_options\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n    - pal_string_to_wstring\n\n[CALL STACK END]\n\nException with an error code: 0xe (SPXERR_MIC_NOT_AVAILABLE)"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "SPEECH_KEY = os.getenv('SPEECH_KEY')\n",
    "SPEECH_REGION = os.getenv('SPEECH_REGION')\n",
    "prompt, _ = recognize_from_microphone(SPEECH_KEY,SPEECH_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = os.getenv('SPEECH_KEY')\n",
    "REGION = os.getenv('SPEECH_REGION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This example requires environment variables named \"SPEECH_KEY\" and \"SPEECH_REGION\"\n",
    "speech_config = speechsdk.SpeechConfig(subscription=KEY, region=REGION)\n",
    "speech_config.speech_recognition_language=\"en-US\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.cognitiveservices.speech.SpeechRecognizer at 0x2527b41ff40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_recognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n"
     ]
    }
   ],
   "source": [
    "print(\"Speak into your microphone.\")\n",
    "speech_recognition_result = speech_recognizer.recognize_once_async().get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recognized: Hello, how are you doing?\n"
     ]
    }
   ],
   "source": [
    "if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "    print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "    print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = speech_recognition_result.cancellation_details\n",
    "    print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "        print(\"Did you set the speech resource key and region values?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.speech import SpeechSynthesisResult\n",
    "import os\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from typing import Optional\n",
    "# Set up logger\n",
    "logger = get_logger()\n",
    "\n",
    "# Load environment variables\n",
    "SPEECH_KEY = os.getenv('SPEECH_KEY')\n",
    "SPEECH_REGION = os.getenv('SPEECH_REGION')\n",
    "\n",
    "def synthesize_speech(text: str, key: str, region: str) -> Optional[SpeechSynthesisResult]:\n",
    "    \"\"\"\n",
    "    Synthesizes speech from the provided text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to synthesize.\n",
    "        key (str): The subscription key for the Speech service.\n",
    "        region (str): The region for the Speech service.\n",
    "\n",
    "    Returns:\n",
    "        Optional[SpeechSynthesisResult]: The result of the speech synthesis.\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key, region=region)\n",
    "    audio_config = speechsdk.audio.AudioOutputConfig(use_default_speaker=True)\n",
    "\n",
    "    # The language of the voice that speaks.\n",
    "    speech_config.speech_synthesis_voice_name='en-US-JennyNeural'\n",
    "\n",
    "    speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    logger.info(f\"Synthesizing speech for text: {text}\")\n",
    "    speech_synthesis_result = speech_synthesizer.speak_text_async(text).get()\n",
    "\n",
    "    if speech_synthesis_result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        logger.info(f\"Speech synthesized for text [{text}]\")\n",
    "    elif speech_synthesis_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_synthesis_result.cancellation_details\n",
    "        logger.error(f\"Speech synthesis canceled: {cancellation_details.reason}\")\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            if cancellation_details.error_details:\n",
    "                logger.error(f\"Error details: {cancellation_details.error_details}\")\n",
    "                logger.error(\"Did you set the speech resource key and region values?\")\n",
    "    \n",
    "    return speech_synthesis_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"What's up Pablo? How are you doing today?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 19:42:44,376 - micro - MainProcess - INFO     Synthesizing speech for text: What's up Pablo? How are you doing today? (3391770958.py:synthesize_speech:32)\n",
      "2023-11-15 19:42:49,065 - micro - MainProcess - INFO     Speech synthesized for text [What's up Pablo? How are you doing today?] (3391770958.py:synthesize_speech:36)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azure.cognitiveservices.speech.SpeechSynthesisResult at 0x1e95a24c110>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthesize_speech(text, SPEECH_KEY, SPEECH_REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Exception with error code: \n[CALL STACK BEGIN]\n\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.extension.audio.sys.so(+0xe1e9) [0x7fd40fc0e1e9]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1f44f9) [0x7fd4249f44f9]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xff4f7) [0x7fd4248ff4f7]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1af42f) [0x7fd4249af42f]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xf108d) [0x7fd4248f108d]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1f44f9) [0x7fd4249f44f9]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xff4f7) [0x7fd4248ff4f7]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1ab483) [0x7fd4249ab483]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x192d81) [0x7fd424992d81]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x130379) [0x7fd424930379]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x130379) [0x7fd424930379]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x19b4f5) [0x7fd42499b4f5]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x13e44f) [0x7fd42493e44f]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1e65f6) [0x7fd4249e65f6]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x139b9b) [0x7fd424939b9b]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x20dfe2) [0x7fd424a0dfe2]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(recognizer_create_speech_recognizer_from_config+0xf2) [0x7fd4248bfc13]\n[CALL STACK END]\n\nException with an error code: 0xe (SPXERR_MIC_NOT_AVAILABLE)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb Cell 4\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m speech_config \u001b[39m=\u001b[39m speechsdk\u001b[39m.\u001b[39mSpeechConfig(subscription\u001b[39m=\u001b[39mKEY, region\u001b[39m=\u001b[39mREGION)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Creates a speech recognizer using microphone as audio input.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# The default language is \"en-us\".\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m speech_recognizer \u001b[39m=\u001b[39m speechsdk\u001b[39m.\u001b[39;49mSpeechRecognizer(speech_config\u001b[39m=\u001b[39;49mspeech_config)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Starts speech recognition, and returns after a single utterance is recognized. The end of a\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# single utterance is determined by listening for silence at the end or until a maximum of 15\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# seconds of audio is processed. It returns the recognition text as result.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Note: Since recognize_once() returns only a single utterance, it is suitable only for single\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# shot recognition like command or query.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# For long-running multi-utterance recognition, use start_continuous_recognition() instead.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#X50sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m result \u001b[39m=\u001b[39m speech_recognizer\u001b[39m.\u001b[39mrecognize_once()\n",
      "File \u001b[0;32m~/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/speech.py:1006\u001b[0m, in \u001b[0;36mSpeechRecognizer.__init__\u001b[0;34m(self, speech_config, audio_config, language, source_language_config, auto_detect_source_language_config)\u001b[0m\n\u001b[1;32m   1004\u001b[0m audio_config_handle \u001b[39m=\u001b[39m audio_config\u001b[39m.\u001b[39m_handle \u001b[39mif\u001b[39;00m audio_config \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[39mif\u001b[39;00m language \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m source_language_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m auto_detect_source_language_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1006\u001b[0m     _call_hr_fn(\n\u001b[1;32m   1007\u001b[0m         fn\u001b[39m=\u001b[39;49m_sdk_lib\u001b[39m.\u001b[39;49mrecognizer_create_speech_recognizer_from_config,\n\u001b[1;32m   1008\u001b[0m         \u001b[39m*\u001b[39;49m[ctypes\u001b[39m.\u001b[39;49mbyref(handle), speech_config\u001b[39m.\u001b[39;49m_handle, audio_config_handle])\n\u001b[1;32m   1009\u001b[0m \u001b[39melif\u001b[39;00m language \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1010\u001b[0m     source_language_config \u001b[39m=\u001b[39m languageconfig\u001b[39m.\u001b[39mSourceLanguageConfig(language)\n",
      "File \u001b[0;32m~/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/interop.py:62\u001b[0m, in \u001b[0;36m_call_hr_fn\u001b[0;34m(fn, *args)\u001b[0m\n\u001b[1;32m     60\u001b[0m fn\u001b[39m.\u001b[39mrestype \u001b[39m=\u001b[39m _spx_hr\n\u001b[1;32m     61\u001b[0m hr \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39margs) \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m fn()\n\u001b[0;32m---> 62\u001b[0m _raise_if_failed(hr)\n",
      "File \u001b[0;32m~/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/interop.py:55\u001b[0m, in \u001b[0;36m_raise_if_failed\u001b[0;34m(hr)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_if_failed\u001b[39m(hr: _spx_hr):\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m hr \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 55\u001b[0m         __try_get_error(_spx_handle(hr))\n\u001b[1;32m     56\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(hr)\n",
      "File \u001b[0;32m~/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/interop.py:50\u001b[0m, in \u001b[0;36m__try_get_error\u001b[0;34m(error_handle)\u001b[0m\n\u001b[1;32m     45\u001b[0m message \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mException with error code: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\n\u001b[1;32m     46\u001b[0m     callstack \u001b[39mif\u001b[39;00m callstack \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     47\u001b[0m     what \u001b[39mif\u001b[39;00m what \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m code\n\u001b[1;32m     48\u001b[0m )\n\u001b[1;32m     49\u001b[0m _sdk_lib\u001b[39m.\u001b[39merror_release(error_handle)\n\u001b[0;32m---> 50\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(message)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception with error code: \n[CALL STACK BEGIN]\n\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.extension.audio.sys.so(+0xe1e9) [0x7fd40fc0e1e9]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1f44f9) [0x7fd4249f44f9]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xff4f7) [0x7fd4248ff4f7]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1af42f) [0x7fd4249af42f]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xf108d) [0x7fd4248f108d]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1f44f9) [0x7fd4249f44f9]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0xff4f7) [0x7fd4248ff4f7]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1ab483) [0x7fd4249ab483]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x192d81) [0x7fd424992d81]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x130379) [0x7fd424930379]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x130379) [0x7fd424930379]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x19b4f5) [0x7fd42499b4f5]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x13e44f) [0x7fd42493e44f]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x1e65f6) [0x7fd4249e65f6]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x139b9b) [0x7fd424939b9b]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(+0x20dfe2) [0x7fd424a0dfe2]\n/home/pablosal/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/libMicrosoft.CognitiveServices.Speech.core.so(recognizer_create_speech_recognizer_from_config+0xf2) [0x7fd4248bfc13]\n[CALL STACK END]\n\nException with an error code: 0xe (SPXERR_MIC_NOT_AVAILABLE)"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"performs one-shot speech recognition from the default microphone\"\"\"\n",
    "# <SpeechRecognitionWithMicrophone>\n",
    "speech_config = speechsdk.SpeechConfig(subscription=KEY, region=REGION)\n",
    "# Creates a speech recognizer using microphone as audio input.\n",
    "# The default language is \"en-us\".\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)\n",
    "\n",
    "# Starts speech recognition, and returns after a single utterance is recognized. The end of a\n",
    "# single utterance is determined by listening for silence at the end or until a maximum of 15\n",
    "# seconds of audio is processed. It returns the recognition text as result.\n",
    "# Note: Since recognize_once() returns only a single utterance, it is suitable only for single\n",
    "# shot recognition like command or query.\n",
    "# For long-running multi-utterance recognition, use start_continuous_recognition() instead.\n",
    "result = speech_recognizer.recognize_once()\n",
    "\n",
    "# Check the result\n",
    "if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "    print(\"Recognized: {}\".format(result.text))\n",
    "elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "    print(\"No speech could be recognized\")\n",
    "elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "    cancellation_details = result.cancellation_details\n",
    "    print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "    if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "        print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "    # </SpeechRecognitionWithMicrophone>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config values\n",
    "with open(r'src/speach_sdk/aoai_config.json') as config_file:\n",
    "    config_details = json.load(config_file)\n",
    "\n",
    "# Setting up the deployment name\n",
    "deployment_name = config_details['COMPLETIONS_MODEL']\n",
    "\n",
    "# This is set to `azure`\n",
    "openai.api_type = \"azure\"\n",
    "\n",
    "# The API key for your Azure OpenAI resource.\n",
    "openai.api_key = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "# The base URL for your Azure OpenAI resource. e.g. \"https://<your resource name>.openai.azure.com\"\n",
    "openai.api_base = config_details['OPENAI_API_BASE']\n",
    "\n",
    "# Currently OPENAI API have the following versions available: 2022-12-01\n",
    "openai.api_version = config_details['OPENAI_API_VERSION']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ml-workspace-dev-eastus-001-aoai.openai.azure.com/'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai.api_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"give me the capital of spain\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Creates a new completion for the provided prompt and parameters.\n",
      "\n",
      "See https://platform.openai.com/docs/api-reference/completions/create for a list\n",
      "of valid parameters.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/openai/api_resources/completion.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "openai.Completion.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Spain is Madrid.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Create a completion for the provided prompt and parameters\n",
    "    # To know more about the parameters, checkout this documentation: https://learn.microsoft.com/en-us/azure/cognitive-services/openai/reference\n",
    "    completion = openai.Completion.create(\n",
    "                    prompt=prompt,\n",
    "                    temperature=0,\n",
    "                    max_tokens=100,\n",
    "                    engine=deployment_name)\n",
    "\n",
    "    # print the completion\n",
    "    print(completion.choices[0].text.strip(\" \\n\"))\n",
    "    \n",
    "    # Here indicating if the response is filtered\n",
    "    if completion.choices[0].finish_reason == \"content_filter\":\n",
    "        print(\"The generated content is filtered.\")\n",
    "        \n",
    "except openai.error.APIError as e:\n",
    "    # Handle API error here, e.g. retry or log\n",
    "    print(f\"OpenAI API returned an API Error: {e}\")\n",
    "\n",
    "except openai.error.AuthenticationError as e:\n",
    "    # Handle Authentication error here, e.g. invalid API key\n",
    "    print(f\"OpenAI API returned an Authentication Error: {e}\")\n",
    "\n",
    "except openai.error.APIConnectionError as e:\n",
    "    # Handle connection error here\n",
    "    print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "\n",
    "except openai.error.InvalidRequestError as e:\n",
    "    # Handle connection error here\n",
    "    print(f\"Invalid Request Error: {e}\")\n",
    "\n",
    "except openai.error.RateLimitError as e:\n",
    "    # Handle rate limit error\n",
    "    print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "\n",
    "except openai.error.ServiceUnavailableError as e:\n",
    "    # Handle Service Unavailable error\n",
    "    print(f\"Service Unavailable: {e}\")\n",
    "\n",
    "except openai.error.Timeout as e:\n",
    "    # Handle request timeout\n",
    "    print(f\"Request timed out: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils.ml_logging import get_logger\n",
    "from src.speach_sdk.speach_to_text import from_file_continous\n",
    "logger = get_logger()\n",
    "import openai\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential = DefaultAzureCredential()\n",
    "token = credential.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "API_KEY = os.getenv('OPENAI_KEY')\n",
    "\n",
    "\n",
    "openai.api_type = \"azure_ad\"\n",
    "openai.api_key = token.token\n",
    "openai.api_base = \"https://example-endpoint.openai.azure.com\"\n",
    "openai.api_version = \"2023-05-15\"  # subject to change\n",
    "\n",
    "\n",
    "ENDPOINT = 'https://ml-workspace-dev-eastus-001-aoai.openai.azure.com/'\n",
    "\n",
    "CONVERSATION = ''\n",
    "\n",
    "PROMPT=f'''Analyze the following conversation, which takes place in a [Context: customer service interaction at a pharmaceutical company]. Focus on identifying the [Intent/Goal: customer's primary concern] related to their inquiry. After analyzing the conversation, provide the findings in [Response Format: a concise summary].\n",
    "\n",
    "Conversation: {CONVERSATION}'''\n",
    "\n",
    "completion = openai.Completion.create(\n",
    "    prompt=\"<prompt>\",\n",
    "    deployment_id=\"text-davinci-003\" # This must match the custom deployment name you chose for your model.\n",
    "    #engine=\"text-davinci-003\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_audio_characteristics(file_name: str):\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_name):\n",
    "        logger.error(f\"File not found: {file_name}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_name)\n",
    "        base_name, _ = os.path.splitext(file_name)\n",
    "        pcm_file_name = base_name + \".pcm\"\n",
    "\n",
    "        audio.export(pcm_file_name, format=\"wav\")\n",
    "        info = mediainfo(pcm_file_name)\n",
    "\n",
    "        logger.info(f\"Audio file characteristics for {pcm_file_name}:\")\n",
    "        logger.info(f\"Number of channels: {info['channels']}\")\n",
    "        if info['bits_per_sample'].isdigit():\n",
    "            sample_width = int(info['bits_per_sample']) / 8\n",
    "            logger.info(f\"Sample width (bytes): {sample_width}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid bits_per_sample value\")\n",
    "        logger.info(f\"Sampling frequency (Hz): {info['sample_rate']}\")\n",
    "        if info['duration'].replace('.', '', 1).isdigit() and info['sample_rate'].isdigit():\n",
    "            number_of_frames = int(float(info['duration']) * int(info['sample_rate']))\n",
    "            logger.info(f\"Number of frames: {number_of_frames}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid duration or sample rate values\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def log_audio_characteristics(file_name: str) -> Optional[None]:\n",
    "    \"\"\"\n",
    "    Logs the characteristics of an audio file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_name):\n",
    "        logger.error(f\"File not found: {file_name}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_name)\n",
    "        base_name, _ = os.path.splitext(file_name)\n",
    "        pcm_file_name = base_name + \".pcm\"\n",
    "\n",
    "        audio.export(pcm_file_name, format=\"wav\")\n",
    "        info = mediainfo(pcm_file_name)\n",
    "\n",
    "        logger.info(f\"Audio file characteristics for {pcm_file_name}:\")\n",
    "        logger.info(f\"Number of channels: {info['channels']}\")\n",
    "        if info['bits_per_sample'].isdigit():\n",
    "            sample_width = int(info['bits_per_sample']) / 8\n",
    "            logger.info(f\"Sample width (bytes): {sample_width}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid bits_per_sample value\")\n",
    "        logger.info(f\"Sampling frequency (Hz): {info['sample_rate']}\")\n",
    "        if info['duration'].replace('.', '', 1).isdigit() and info['sample_rate'].isdigit():\n",
    "            number_of_frames = int(float(info['duration']) * int(info['sample_rate']))\n",
    "            logger.info(f\"Number of frames: {number_of_frames}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid duration or sample rate values\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def log_audio_characteristics(file_name: str) -> Optional[None]:\n",
    "    \"\"\"\n",
    "    Logs the characteristics of an audio file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_name):\n",
    "        logger.error(f\"File not found: {file_name}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_name)\n",
    "        base_name, _ = os.path.splitext(file_name)\n",
    "        pcm_file_name = base_name + \".pcm\"\n",
    "\n",
    "        audio.export(pcm_file_name, format=\"wav\")\n",
    "        info = mediainfo(pcm_file_name)\n",
    "\n",
    "        logger.info(f\"Audio file characteristics for {pcm_file_name}:\")\n",
    "        logger.info(f\"Number of channels: {info['channels']}\")\n",
    "        if info['bits_per_sample'].isdigit():\n",
    "            sample_width = int(info['bits_per_sample']) / 8\n",
    "            logger.info(f\"Sample width (bytes): {sample_width}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid bits_per_sample value\")\n",
    "        logger.info(f\"Sampling frequency (Hz): {info['sample_rate']}\")\n",
    "        if info['duration'].replace('.', '', 1).isdigit() and info['sample_rate'].isdigit():\n",
    "            number_of_frames = int(float(info['duration']) * int(info['sample_rate']))\n",
    "            logger.info(f\"Number of frames: {number_of_frames}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid duration or sample rate values\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = os.getenv('KEY')\n",
    "REGION = os.getenv('REGION')\n",
    "FILE_NAME = '/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/8000khz-mulaw-pullstream/7.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 08:02:35,611 - micro - MainProcess - INFO     Audio file characteristics for /mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/8000khz-mulaw-pullstream/7.pcm: (2467322109.py:log_audio_characteristics:15)\n",
      "2023-11-15 08:02:35,612 - micro - MainProcess - INFO     Number of channels: 2 (2467322109.py:log_audio_characteristics:16)\n",
      "2023-11-15 08:02:35,613 - micro - MainProcess - INFO     Sample width (bytes): 1.0 (2467322109.py:log_audio_characteristics:19)\n",
      "2023-11-15 08:02:35,614 - micro - MainProcess - INFO     Sampling frequency (Hz): 8000 (2467322109.py:log_audio_characteristics:22)\n",
      "2023-11-15 08:02:35,615 - micro - MainProcess - INFO     Number of frames: 182880 (2467322109.py:log_audio_characteristics:25)\n"
     ]
    }
   ],
   "source": [
    "log_audio_characteristics(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_file_async(file_name: str, key: str, region: str) -> str:\n",
    "    \"\"\"\n",
    "    Transcribes speech from an audio file using Azure Cognitive Services Speech SDK.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the audio file to transcribe.\n",
    "        key (str): The subscription key for the Speech service.\n",
    "        region (str): The region for the Speech service.\n",
    "\n",
    "    Returns:\n",
    "        str: The transcribed text from the audio file.\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key, region=region)\n",
    "    audio_config = speechsdk.AudioConfig(filename=file_name)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    logger.info(f\"Transcribing speech from file: {file_name}\")\n",
    "    result = speech_recognizer.recognize_once_async().get()\n",
    "    if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        logger.info(f\"Transcription result: {result.text}\")\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        logger.warning(f\"No speech could be recognized: {result.no_match_details}\")\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        logger.error(f\"Speech Recognition canceled: {cancellation_details.reason}\")\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            logger.error(f\"Error details: {cancellation_details.error_details}\")\n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def from_file_continous(file_name: str, key: str, region: str) -> str:\n",
    "    \"\"\"performs continuous speech recognition with input from an audio file\"\"\"\n",
    "    # Set up logging\n",
    "  \n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key, region=region)\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=file_name)\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    log_audio_characteristics(file_name)\n",
    "\n",
    "    done = False\n",
    "    final_text = \"\"\n",
    "\n",
    "    def update_final_text(evt):\n",
    "        nonlocal final_text\n",
    "        final_text += ' ' + evt.result.text\n",
    "\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        logger.info('CLOSING on {}'.format(evt))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    # Connect callbacks to the events fired by the speech recognizer\n",
    "    speech_recognizer.recognizing.connect(lambda evt: logger.info('RECOGNIZING: {}'.format(evt)))\n",
    "    speech_recognizer.recognized.connect(update_final_text)\n",
    "    speech_recognizer.session_started.connect(lambda evt: logger.info('SESSION STARTED: {}'.format(evt)))\n",
    "    speech_recognizer.session_stopped.connect(lambda evt: logger.info('SESSION STOPPED {}'.format(evt)))\n",
    "    speech_recognizer.canceled.connect(lambda evt: logger.info('CANCELED {}'.format(evt)))\n",
    "    # Stop continuous recognition on either session stopped or canceled events\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start continuous speech recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.1)\n",
    "\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "    return final_text.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "either subscription key or authorization token must be given along with a region",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m from_file_continous(file_name\u001b[39m=\u001b[39;49mFILE_NAME, key\u001b[39m=\u001b[39;49mKEY, region\u001b[39m=\u001b[39;49mREGION)\n",
      "\u001b[1;32m/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"performs continuous speech recognition with input from an audio file\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Set up logging\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m speech_config \u001b[39m=\u001b[39m speechsdk\u001b[39m.\u001b[39;49mSpeechConfig(subscription\u001b[39m=\u001b[39;49mkey, region\u001b[39m=\u001b[39;49mregion)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m audio_config \u001b[39m=\u001b[39m speechsdk\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mAudioConfig(filename\u001b[39m=\u001b[39mfile_name)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m speech_recognizer \u001b[39m=\u001b[39m speechsdk\u001b[39m.\u001b[39mSpeechRecognizer(speech_config\u001b[39m=\u001b[39mspeech_config, audio_config\u001b[39m=\u001b[39maudio_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/speech.py:59\u001b[0m, in \u001b[0;36mSpeechConfig.__init__\u001b[0;34m(self, subscription, region, endpoint, host, auth_token, speech_recognition_language)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mcannot specify both auth_token and endpoint or host when constructing SpeechConfig. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     56\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39mSet authorization token separately after creating SpeechConfig.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m region \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m subscription \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m auth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meither subscription key or authorization token must be given along with a region\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m subscription \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m endpoint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m host \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m region \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meither endpoint, host, or region must be given along with a subscription key\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: either subscription key or authorization token must be given along with a region"
     ]
    }
   ],
   "source": [
    "text = from_file_continous(file_name=FILE_NAME, key=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the date? May 15th, 1980. Thursday, May 15th, 19180. What is the date? July 6th. Saturday, July 6th, 2024.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 16:46:38,782 - micro - MainProcess - INFO     Transcribing speech from file: /mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/8000khz-mulaw-pullstream/7.wav (248621148.py:from_file:17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 16:46:39,395 - micro - MainProcess - INFO     Transcription result: What is the date? (248621148.py:from_file:20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the date?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_file(file_name=FILE_NAME, key=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config.enable_audio_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = speechsdk.AudioConfig(filename=FILE_NAME)\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mspeech_recognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mazure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcognitiveservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpeechRecognitionResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Performs recognition in a blocking (synchronous) mode. Returns after a single utterance is\n",
      "recognized. The end of a single utterance is determined by listening for silence at the end\n",
      "or until a maximum of 15 seconds of audio is processed. The task returns the recognition\n",
      "text as result. For long-running multi-utterance recognition, use\n",
      ":py:meth:`.start_continuous_recognition_async` instead.\n",
      "\n",
      ":returns: The result value of the synchronous recognition.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/speech.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "result = speech_recognizer.recognize_once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the date?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Id\":\"0a812f792b094255b31eb6e1abf81c88\",\"RecognitionStatus\":\"Success\",\"DisplayText\":\"May 15th, 1980.\",\"Offset\":38600000,\"Duration\":16000000,\"Channel\":0}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the date?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time \n",
    "import os\n",
    "from utils.ml_logging import get_logger\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "KEY = os.getenv('KEY')\n",
    "REGION = os.getenv('REGION')\n",
    "FILE_NAME = '/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/8000khz-mulaw-pullstream/7.wav'\n",
    "language_understanding_app_id = os.getenv('INTENT_KEY')\n",
    "\n",
    "\n",
    "\n",
    "intent_config: speechsdk.SpeechConfig = speechsdk.SpeechConfig(subscription=KEY, region=REGION)\n",
    "audio_config: speechsdk.audio.AudioConfig = speechsdk.audio.AudioConfig(filename=FILE_NAME)\n",
    "intent_recognizer: speechsdk.intent.IntentRecognizer = speechsdk.intent.IntentRecognizer(speech_config=intent_config, audio_config=audio_config)\n",
    "\n",
    "# set up the intents that are to be recognized. These can be a mix of simple phrases and\n",
    "# intents specified through a LanguageUnderstanding Model.\n",
    "model = speechsdk.intent.LanguageUnderstandingModel(app_id=language_understanding_app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_intent_continuous(file_name: str, key: str, region: str) -> None:\n",
    "    \"\"\"\n",
    "    Performs continuous intent recognition from input from an audio file.\n",
    "    Uses the Azure Cognitive Services Speech SDK to set up an intent recognizer,\n",
    "    add intents to be recognized, and start continuous recognition.\n",
    "    Prints the output of the recognition to the console.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the audio file to transcribe.\n",
    "        key (str): The subscription key for the Speech service.\n",
    "        region (str): The region for the Speech service.\n",
    "    \"\"\"\n",
    "    # Set up the intent recognizer\n",
    "    intent_config: speechsdk.SpeechConfig = speechsdk.SpeechConfig(subscription=key, region=region)\n",
    "    audio_config: speechsdk.audio.AudioConfig = speechsdk.audio.AudioConfig(filename=file_name)\n",
    "    intent_recognizer: speechsdk.intent.IntentRecognizer = speechsdk.intent.IntentRecognizer(speech_config=intent_config, audio_config=audio_config)\n",
    "\n",
    "    # set up the intents that are to be recognized. These can be a mix of simple phrases and\n",
    "    # intents specified through a LanguageUnderstanding Model.\n",
    "    model = speechsdk.intent.LanguageUnderstandingModel(app_id=language_understanding_app_id)\n",
    "    intents = [\n",
    "        (model, \"HomeAutomation.TurnOn\"),\n",
    "        (model, \"HomeAutomation.TurnOff\"),\n",
    "        (\"This is a test.\", \"test\"),\n",
    "        (\"Switch the channel to 34.\", \"34\"),\n",
    "        (\"what's the weather like\", \"weather\"),\n",
    "    ]\n",
    "    intent_recognizer.add_intents(intents)\n",
    "\n",
    "    # Connect callback functions to the signals the intent recognizer fires.\n",
    "    done = False\n",
    "\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    intent_recognizer.session_started.connect(lambda evt: print(\"SESSION_START: {}\".format(evt)))\n",
    "    intent_recognizer.speech_end_detected.connect(lambda evt: print(\"SPEECH_END_DETECTED: {}\".format(evt)))\n",
    "    # event for intermediate results\n",
    "    intent_recognizer.recognizing.connect(lambda evt: print(\"RECOGNIZING: {}\".format(evt)))\n",
    "    # event for final result\n",
    "    intent_recognizer.recognized.connect(lambda evt: print(\n",
    "        \"RECOGNIZED: {}\\n\\tText: {} (Reason: {})\\n\\tIntent Id: {}\\n\\tIntent JSON: {}\".format(\n",
    "            evt, evt.result.text, evt.result.reason, evt.result.intent_id, evt.result.intent_json)))\n",
    "\n",
    "    # cancellation event\n",
    "    intent_recognizer.canceled.connect(lambda evt: print(f\"CANCELED: {evt.cancellation_details} ({evt.reason})\"))\n",
    "\n",
    "    # stop continuous recognition on session stopped, end of speech or canceled events\n",
    "    intent_recognizer.session_stopped.connect(stop_cb)\n",
    "    intent_recognizer.speech_end_detected.connect(stop_cb)\n",
    "    intent_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # And finally run the intent recognizer. The output of the callbacks should be printed to the console.\n",
    "    intent_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.5)\n",
    "\n",
    "    intent_recognizer.stop_continuous_recognition()\n",
    "    # </IntentContinuousRecognitionWithFile>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SESSION_START: SessionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=38a249704fe3412385174092ed0eb4f1, text=\"what is the\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "SPEECH_END_DETECTED: RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "CLOSING on RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "RECOGNIZED: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=04c7e8e4b9c747439abfaa2cc6183ecc, text=\"What is the date?\", intent_id=, reason=ResultReason.RecognizedSpeech))\n",
      "\tText: What is the date? (Reason: ResultReason.RecognizedSpeech)\n",
      "\tIntent Id: \n",
      "\tIntent JSON: \n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=69f1ceb0bf0b43bebe3a27ef0903d909, text=\"may\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=01e2938ae8014c07991541e4eccb20d6, text=\"may 15th\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=6aa6aa3cb71e439889fc4187d33487c8, text=\"may 15th 1980\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "SPEECH_END_DETECTED: RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "CLOSING on RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "RECOGNIZED: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=7dad3d545cac44508b20a6feb210d61e, text=\"May 15th, 1980.\", intent_id=, reason=ResultReason.RecognizedSpeech))\n",
      "\tText: May 15th, 1980. (Reason: ResultReason.RecognizedSpeech)\n",
      "\tIntent Id: \n",
      "\tIntent JSON: \n",
      "CLOSING on SessionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n"
     ]
    }
   ],
   "source": [
    "recognize_intent_continuous(file_name=FILE_NAME, key=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = [\n",
    "        (model, \"HomeAutomation.TurnOn\"),\n",
    "        (model, \"HomeAutomation.TurnOff\"),\n",
    "        (\"This is a test.\", \"test\"),\n",
    "        (\"Switch the channel to 34.\", \"34\"),\n",
    "        (\"what's the weather like\", \"weather\"),\n",
    "    ]\n",
    "intent_recognizer.add_intents(intents)\n",
    "\n",
    "# Connect callback functions to the signals the intent recognizer fires.\n",
    "done = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "    \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "    print('CLOSING on {}'.format(evt))\n",
    "    nonlocal done\n",
    "    done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "intent_recognizer.session_started.connect(lambda evt: print(\"SESSION_START: {}\".format(evt)))\n",
    "intent_recognizer.speech_end_detected.connect(lambda evt: print(\"SPEECH_END_DETECTED: {}\".format(evt)))\n",
    "# event for intermediate results\n",
    "intent_recognizer.recognizing.connect(lambda evt: print(\"RECOGNIZING: {}\".format(evt)))\n",
    "# event for final result\n",
    "intent_recognizer.recognized.connect(lambda evt: print(\n",
    "    \"RECOGNIZED: {}\\n\\tText: {} (Reason: {})\\n\\tIntent Id: {}\\n\\tIntent JSON: {}\".format(\n",
    "        evt, evt.result.text, evt.result.reason, evt.result.intent_id, evt.result.intent_json)))\n",
    "\n",
    "# cancellation event\n",
    "intent_recognizer.canceled.connect(lambda evt: print(f\"CANCELED: {evt.cancellation_details} ({evt.reason})\"))\n",
    "\n",
    "# stop continuous recognition on session stopped, end of speech or canceled events\n",
    "intent_recognizer.session_stopped.connect(stop_cb)\n",
    "intent_recognizer.speech_end_detected.connect(stop_cb)\n",
    "intent_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "# And finally run the intent recognizer. The output of the callbacks should be printed to the console.\n",
    "intent_recognizer.start_continuous_recognition()\n",
    "while not done:\n",
    "    time.sleep(.5)\n",
    "\n",
    "intent_recognizer.stop_continuous_recognition()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
