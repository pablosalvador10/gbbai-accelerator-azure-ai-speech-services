{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set the working directory to your project's main folder\n",
    "os.chdir('/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/')\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from utils.ml_logging import get_logger\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "logger = get_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.utils import mediainfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_audio_characteristics(file_name: str):\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_name):\n",
    "        logger.error(f\"File not found: {file_name}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_name)\n",
    "        base_name, _ = os.path.splitext(file_name)\n",
    "        pcm_file_name = base_name + \".pcm\"\n",
    "\n",
    "        audio.export(pcm_file_name, format=\"wav\")\n",
    "        info = mediainfo(pcm_file_name)\n",
    "\n",
    "        logger.info(f\"Audio file characteristics for {pcm_file_name}:\")\n",
    "        logger.info(f\"Number of channels: {info['channels']}\")\n",
    "        if info['bits_per_sample'].isdigit():\n",
    "            sample_width = int(info['bits_per_sample']) / 8\n",
    "            logger.info(f\"Sample width (bytes): {sample_width}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid bits_per_sample value\")\n",
    "        logger.info(f\"Sampling frequency (Hz): {info['sample_rate']}\")\n",
    "        if info['duration'].replace('.', '', 1).isdigit() and info['sample_rate'].isdigit():\n",
    "            number_of_frames = int(float(info['duration']) * int(info['sample_rate']))\n",
    "            logger.info(f\"Number of frames: {number_of_frames}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid duration or sample rate values\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def log_audio_characteristics(file_name: str) -> Optional[None]:\n",
    "    \"\"\"\n",
    "    Logs the characteristics of an audio file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_name):\n",
    "        logger.error(f\"File not found: {file_name}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_name)\n",
    "        base_name, _ = os.path.splitext(file_name)\n",
    "        pcm_file_name = base_name + \".pcm\"\n",
    "\n",
    "        audio.export(pcm_file_name, format=\"wav\")\n",
    "        info = mediainfo(pcm_file_name)\n",
    "\n",
    "        logger.info(f\"Audio file characteristics for {pcm_file_name}:\")\n",
    "        logger.info(f\"Number of channels: {info['channels']}\")\n",
    "        if info['bits_per_sample'].isdigit():\n",
    "            sample_width = int(info['bits_per_sample']) / 8\n",
    "            logger.info(f\"Sample width (bytes): {sample_width}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid bits_per_sample value\")\n",
    "        logger.info(f\"Sampling frequency (Hz): {info['sample_rate']}\")\n",
    "        if info['duration'].replace('.', '', 1).isdigit() and info['sample_rate'].isdigit():\n",
    "            number_of_frames = int(float(info['duration']) * int(info['sample_rate']))\n",
    "            logger.info(f\"Number of frames: {number_of_frames}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid duration or sample rate values\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def log_audio_characteristics(file_name: str) -> Optional[None]:\n",
    "    \"\"\"\n",
    "    Logs the characteristics of an audio file.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The path to the audio file.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_name):\n",
    "        logger.error(f\"File not found: {file_name}\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_name)\n",
    "        base_name, _ = os.path.splitext(file_name)\n",
    "        pcm_file_name = base_name + \".pcm\"\n",
    "\n",
    "        audio.export(pcm_file_name, format=\"wav\")\n",
    "        info = mediainfo(pcm_file_name)\n",
    "\n",
    "        logger.info(f\"Audio file characteristics for {pcm_file_name}:\")\n",
    "        logger.info(f\"Number of channels: {info['channels']}\")\n",
    "        if info['bits_per_sample'].isdigit():\n",
    "            sample_width = int(info['bits_per_sample']) / 8\n",
    "            logger.info(f\"Sample width (bytes): {sample_width}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid bits_per_sample value\")\n",
    "        logger.info(f\"Sampling frequency (Hz): {info['sample_rate']}\")\n",
    "        if info['duration'].replace('.', '', 1).isdigit() and info['sample_rate'].isdigit():\n",
    "            number_of_frames = int(float(info['duration']) * int(info['sample_rate']))\n",
    "            logger.info(f\"Number of frames: {number_of_frames}\")\n",
    "        else:\n",
    "            logger.error(\"Invalid duration or sample rate values\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = os.getenv('KEY')\n",
    "REGION = os.getenv('REGION')\n",
    "FILE_NAME = '/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/8000khz-mulaw-pullstream/7.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-15 08:02:35,611 - micro - MainProcess - INFO     Audio file characteristics for /mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/8000khz-mulaw-pullstream/7.pcm: (2467322109.py:log_audio_characteristics:15)\n",
      "2023-11-15 08:02:35,612 - micro - MainProcess - INFO     Number of channels: 2 (2467322109.py:log_audio_characteristics:16)\n",
      "2023-11-15 08:02:35,613 - micro - MainProcess - INFO     Sample width (bytes): 1.0 (2467322109.py:log_audio_characteristics:19)\n",
      "2023-11-15 08:02:35,614 - micro - MainProcess - INFO     Sampling frequency (Hz): 8000 (2467322109.py:log_audio_characteristics:22)\n",
      "2023-11-15 08:02:35,615 - micro - MainProcess - INFO     Number of frames: 182880 (2467322109.py:log_audio_characteristics:25)\n"
     ]
    }
   ],
   "source": [
    "log_audio_characteristics(FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_file_async(file_name: str, key: str, region: str) -> str:\n",
    "    \"\"\"\n",
    "    Transcribes speech from an audio file using Azure Cognitive Services Speech SDK.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the audio file to transcribe.\n",
    "        key (str): The subscription key for the Speech service.\n",
    "        region (str): The region for the Speech service.\n",
    "\n",
    "    Returns:\n",
    "        str: The transcribed text from the audio file.\n",
    "    \"\"\"\n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key, region=region)\n",
    "    audio_config = speechsdk.AudioConfig(filename=file_name)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    logger.info(f\"Transcribing speech from file: {file_name}\")\n",
    "    result = speech_recognizer.recognize_once_async().get()\n",
    "    if result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        logger.info(f\"Transcription result: {result.text}\")\n",
    "    elif result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        logger.warning(f\"No speech could be recognized: {result.no_match_details}\")\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        logger.error(f\"Speech Recognition canceled: {cancellation_details.reason}\")\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            logger.error(f\"Error details: {cancellation_details.error_details}\")\n",
    "    return result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def from_file_continous(file_name: str, key: str, region: str) -> str:\n",
    "    \"\"\"performs continuous speech recognition with input from an audio file\"\"\"\n",
    "    # Set up logging\n",
    "  \n",
    "    speech_config = speechsdk.SpeechConfig(subscription=key, region=region)\n",
    "    audio_config = speechsdk.audio.AudioConfig(filename=file_name)\n",
    "\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    log_audio_characteristics(file_name)\n",
    "\n",
    "    done = False\n",
    "    final_text = \"\"\n",
    "\n",
    "    def update_final_text(evt):\n",
    "        nonlocal final_text\n",
    "        final_text += ' ' + evt.result.text\n",
    "\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        logger.info('CLOSING on {}'.format(evt))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    # Connect callbacks to the events fired by the speech recognizer\n",
    "    speech_recognizer.recognizing.connect(lambda evt: logger.info('RECOGNIZING: {}'.format(evt)))\n",
    "    speech_recognizer.recognized.connect(update_final_text)\n",
    "    speech_recognizer.session_started.connect(lambda evt: logger.info('SESSION STARTED: {}'.format(evt)))\n",
    "    speech_recognizer.session_stopped.connect(lambda evt: logger.info('SESSION STOPPED {}'.format(evt)))\n",
    "    speech_recognizer.canceled.connect(lambda evt: logger.info('CANCELED {}'.format(evt)))\n",
    "    # Stop continuous recognition on either session stopped or canceled events\n",
    "    speech_recognizer.session_stopped.connect(stop_cb)\n",
    "    speech_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # Start continuous speech recognition\n",
    "    speech_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.1)\n",
    "\n",
    "    speech_recognizer.stop_continuous_recognition()\n",
    "\n",
    "    return final_text.strip() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "either subscription key or authorization token must be given along with a region",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m text \u001b[39m=\u001b[39m from_file_continous(file_name\u001b[39m=\u001b[39;49mFILE_NAME, key\u001b[39m=\u001b[39;49mKEY, region\u001b[39m=\u001b[39;49mREGION)\n",
      "\u001b[1;32m/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb Cell 8\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"performs continuous speech recognition with input from an audio file\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Set up logging\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m speech_config \u001b[39m=\u001b[39m speechsdk\u001b[39m.\u001b[39;49mSpeechConfig(subscription\u001b[39m=\u001b[39;49mkey, region\u001b[39m=\u001b[39;49mregion)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m audio_config \u001b[39m=\u001b[39m speechsdk\u001b[39m.\u001b[39maudio\u001b[39m.\u001b[39mAudioConfig(filename\u001b[39m=\u001b[39mfile_name)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/testing.ipynb#W4sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m speech_recognizer \u001b[39m=\u001b[39m speechsdk\u001b[39m.\u001b[39mSpeechRecognizer(speech_config\u001b[39m=\u001b[39mspeech_config, audio_config\u001b[39m=\u001b[39maudio_config)\n",
      "File \u001b[0;32m~/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/speech.py:59\u001b[0m, in \u001b[0;36mSpeechConfig.__init__\u001b[0;34m(self, subscription, region, endpoint, host, auth_token, speech_recognition_language)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mcannot specify both auth_token and endpoint or host when constructing SpeechConfig. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     56\u001b[0m                          \u001b[39m'\u001b[39m\u001b[39mSet authorization token separately after creating SpeechConfig.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[39mif\u001b[39;00m region \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m subscription \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m auth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meither subscription key or authorization token must be given along with a region\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[39mif\u001b[39;00m subscription \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m endpoint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m host \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m region \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39meither endpoint, host, or region must be given along with a subscription key\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: either subscription key or authorization token must be given along with a region"
     ]
    }
   ],
   "source": [
    "text = from_file_continous(file_name=FILE_NAME, key=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the date? May 15th, 1980. Thursday, May 15th, 19180. What is the date? July 6th. Saturday, July 6th, 2024.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 16:46:38,782 - micro - MainProcess - INFO     Transcribing speech from file: /mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/8000khz-mulaw-pullstream/7.wav (248621148.py:from_file:17)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-14 16:46:39,395 - micro - MainProcess - INFO     Transcription result: What is the date? (248621148.py:from_file:20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'What is the date?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from_file(file_name=FILE_NAME, key=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config = speechsdk.SpeechConfig(subscription=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config.enable_audio_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = speechsdk.AudioConfig(filename=FILE_NAME)\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mspeech_recognizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecognize_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mazure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcognitiveservices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspeech\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSpeechRecognitionResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Performs recognition in a blocking (synchronous) mode. Returns after a single utterance is\n",
      "recognized. The end of a single utterance is determined by listening for silence at the end\n",
      "or until a maximum of 15 seconds of audio is processed. The task returns the recognition\n",
      "text as result. For long-running multi-utterance recognition, use\n",
      ":py:meth:`.start_continuous_recognition_async` instead.\n",
      "\n",
      ":returns: The result value of the synchronous recognition.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/lilly-speach-to-text/lib/python3.9/site-packages/azure/cognitiveservices/speech/speech.py\n",
      "\u001b[0;31mType:\u001b[0m      method"
     ]
    }
   ],
   "source": [
    "result = speech_recognizer.recognize_once?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the date?'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"Id\":\"0a812f792b094255b31eb6e1abf81c88\",\"RecognitionStatus\":\"Success\",\"DisplayText\":\"May 15th, 1980.\",\"Offset\":38600000,\"Duration\":16000000,\"Channel\":0}'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is the date?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import time \n",
    "import os\n",
    "from utils.ml_logging import get_logger\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "logger = get_logger()\n",
    "\n",
    "\n",
    "KEY = os.getenv('KEY')\n",
    "REGION = os.getenv('REGION')\n",
    "FILE_NAME = '/mnt/c/Users/pablosal/Desktop/lilly-workshop-gbb-text-to-speach/notebooks/dev/8000khz-mulaw-pullstream/7.wav'\n",
    "language_understanding_app_id = os.getenv('INTENT_KEY')\n",
    "\n",
    "\n",
    "\n",
    "intent_config: speechsdk.SpeechConfig = speechsdk.SpeechConfig(subscription=KEY, region=REGION)\n",
    "audio_config: speechsdk.audio.AudioConfig = speechsdk.audio.AudioConfig(filename=FILE_NAME)\n",
    "intent_recognizer: speechsdk.intent.IntentRecognizer = speechsdk.intent.IntentRecognizer(speech_config=intent_config, audio_config=audio_config)\n",
    "\n",
    "# set up the intents that are to be recognized. These can be a mix of simple phrases and\n",
    "# intents specified through a LanguageUnderstanding Model.\n",
    "model = speechsdk.intent.LanguageUnderstandingModel(app_id=language_understanding_app_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_intent_continuous(file_name: str, key: str, region: str) -> None:\n",
    "    \"\"\"\n",
    "    Performs continuous intent recognition from input from an audio file.\n",
    "    Uses the Azure Cognitive Services Speech SDK to set up an intent recognizer,\n",
    "    add intents to be recognized, and start continuous recognition.\n",
    "    Prints the output of the recognition to the console.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the audio file to transcribe.\n",
    "        key (str): The subscription key for the Speech service.\n",
    "        region (str): The region for the Speech service.\n",
    "    \"\"\"\n",
    "    # Set up the intent recognizer\n",
    "    intent_config: speechsdk.SpeechConfig = speechsdk.SpeechConfig(subscription=key, region=region)\n",
    "    audio_config: speechsdk.audio.AudioConfig = speechsdk.audio.AudioConfig(filename=file_name)\n",
    "    intent_recognizer: speechsdk.intent.IntentRecognizer = speechsdk.intent.IntentRecognizer(speech_config=intent_config, audio_config=audio_config)\n",
    "\n",
    "    # set up the intents that are to be recognized. These can be a mix of simple phrases and\n",
    "    # intents specified through a LanguageUnderstanding Model.\n",
    "    model = speechsdk.intent.LanguageUnderstandingModel(app_id=language_understanding_app_id)\n",
    "    intents = [\n",
    "        (model, \"HomeAutomation.TurnOn\"),\n",
    "        (model, \"HomeAutomation.TurnOff\"),\n",
    "        (\"This is a test.\", \"test\"),\n",
    "        (\"Switch the channel to 34.\", \"34\"),\n",
    "        (\"what's the weather like\", \"weather\"),\n",
    "    ]\n",
    "    intent_recognizer.add_intents(intents)\n",
    "\n",
    "    # Connect callback functions to the signals the intent recognizer fires.\n",
    "    done = False\n",
    "\n",
    "    def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "        \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "        print('CLOSING on {}'.format(evt))\n",
    "        nonlocal done\n",
    "        done = True\n",
    "\n",
    "    intent_recognizer.session_started.connect(lambda evt: print(\"SESSION_START: {}\".format(evt)))\n",
    "    intent_recognizer.speech_end_detected.connect(lambda evt: print(\"SPEECH_END_DETECTED: {}\".format(evt)))\n",
    "    # event for intermediate results\n",
    "    intent_recognizer.recognizing.connect(lambda evt: print(\"RECOGNIZING: {}\".format(evt)))\n",
    "    # event for final result\n",
    "    intent_recognizer.recognized.connect(lambda evt: print(\n",
    "        \"RECOGNIZED: {}\\n\\tText: {} (Reason: {})\\n\\tIntent Id: {}\\n\\tIntent JSON: {}\".format(\n",
    "            evt, evt.result.text, evt.result.reason, evt.result.intent_id, evt.result.intent_json)))\n",
    "\n",
    "    # cancellation event\n",
    "    intent_recognizer.canceled.connect(lambda evt: print(f\"CANCELED: {evt.cancellation_details} ({evt.reason})\"))\n",
    "\n",
    "    # stop continuous recognition on session stopped, end of speech or canceled events\n",
    "    intent_recognizer.session_stopped.connect(stop_cb)\n",
    "    intent_recognizer.speech_end_detected.connect(stop_cb)\n",
    "    intent_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "    # And finally run the intent recognizer. The output of the callbacks should be printed to the console.\n",
    "    intent_recognizer.start_continuous_recognition()\n",
    "    while not done:\n",
    "        time.sleep(.5)\n",
    "\n",
    "    intent_recognizer.stop_continuous_recognition()\n",
    "    # </IntentContinuousRecognitionWithFile>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SESSION_START: SessionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=38a249704fe3412385174092ed0eb4f1, text=\"what is the\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "SPEECH_END_DETECTED: RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "CLOSING on RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "RECOGNIZED: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=04c7e8e4b9c747439abfaa2cc6183ecc, text=\"What is the date?\", intent_id=, reason=ResultReason.RecognizedSpeech))\n",
      "\tText: What is the date? (Reason: ResultReason.RecognizedSpeech)\n",
      "\tIntent Id: \n",
      "\tIntent JSON: \n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=69f1ceb0bf0b43bebe3a27ef0903d909, text=\"may\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=01e2938ae8014c07991541e4eccb20d6, text=\"may 15th\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "RECOGNIZING: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=6aa6aa3cb71e439889fc4187d33487c8, text=\"may 15th 1980\", intent_id=, reason=ResultReason.RecognizingSpeech))\n",
      "SPEECH_END_DETECTED: RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "CLOSING on RecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n",
      "RECOGNIZED: IntentRecognitionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a, result=IntentRecognitionResult(result_id=7dad3d545cac44508b20a6feb210d61e, text=\"May 15th, 1980.\", intent_id=, reason=ResultReason.RecognizedSpeech))\n",
      "\tText: May 15th, 1980. (Reason: ResultReason.RecognizedSpeech)\n",
      "\tIntent Id: \n",
      "\tIntent JSON: \n",
      "CLOSING on SessionEventArgs(session_id=5148196610eb42618ea78e19bb722d2a)\n"
     ]
    }
   ],
   "source": [
    "recognize_intent_continuous(file_name=FILE_NAME, key=KEY, region=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "intents = [\n",
    "        (model, \"HomeAutomation.TurnOn\"),\n",
    "        (model, \"HomeAutomation.TurnOff\"),\n",
    "        (\"This is a test.\", \"test\"),\n",
    "        (\"Switch the channel to 34.\", \"34\"),\n",
    "        (\"what's the weather like\", \"weather\"),\n",
    "    ]\n",
    "intent_recognizer.add_intents(intents)\n",
    "\n",
    "# Connect callback functions to the signals the intent recognizer fires.\n",
    "done = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_cb(evt: speechsdk.SessionEventArgs):\n",
    "    \"\"\"callback that signals to stop continuous recognition upon receiving an event `evt`\"\"\"\n",
    "    print('CLOSING on {}'.format(evt))\n",
    "    nonlocal done\n",
    "    done = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "intent_recognizer.session_started.connect(lambda evt: print(\"SESSION_START: {}\".format(evt)))\n",
    "intent_recognizer.speech_end_detected.connect(lambda evt: print(\"SPEECH_END_DETECTED: {}\".format(evt)))\n",
    "# event for intermediate results\n",
    "intent_recognizer.recognizing.connect(lambda evt: print(\"RECOGNIZING: {}\".format(evt)))\n",
    "# event for final result\n",
    "intent_recognizer.recognized.connect(lambda evt: print(\n",
    "    \"RECOGNIZED: {}\\n\\tText: {} (Reason: {})\\n\\tIntent Id: {}\\n\\tIntent JSON: {}\".format(\n",
    "        evt, evt.result.text, evt.result.reason, evt.result.intent_id, evt.result.intent_json)))\n",
    "\n",
    "# cancellation event\n",
    "intent_recognizer.canceled.connect(lambda evt: print(f\"CANCELED: {evt.cancellation_details} ({evt.reason})\"))\n",
    "\n",
    "# stop continuous recognition on session stopped, end of speech or canceled events\n",
    "intent_recognizer.session_stopped.connect(stop_cb)\n",
    "intent_recognizer.speech_end_detected.connect(stop_cb)\n",
    "intent_recognizer.canceled.connect(stop_cb)\n",
    "\n",
    "# And finally run the intent recognizer. The output of the callbacks should be printed to the console.\n",
    "intent_recognizer.start_continuous_recognition()\n",
    "while not done:\n",
    "    time.sleep(.5)\n",
    "\n",
    "intent_recognizer.stop_continuous_recognition()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 ('my-template-environment')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7cde2d05b67b86bafb567671a494f38119b1228112c72ed3f4909b1daf51dd2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
